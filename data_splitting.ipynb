{"cells":[{"cell_type":"markdown","source":["# **Import Libraries**"],"metadata":{"id":"2H-utVN0C7Yz"},"id":"2H-utVN0C7Yz"},{"cell_type":"code","source":["import os\n","import json\n","import shutil\n","from tqdm import tqdm\n","import yaml"],"metadata":{"id":"QFlX8we7DFQE"},"id":"QFlX8we7DFQE","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Define all the file paths**"],"metadata":{"id":"OQG9FIplDGDz"},"id":"OQG9FIplDGDz"},{"cell_type":"code","execution_count":null,"id":"b1493765","metadata":{"id":"b1493765"},"outputs":[],"source":["# Dataset path\n","\n","# Dataset in the same directory\n","current_dir = os.getcwd()\n","DATASET_ROOT = os.path.join(current_dir, \"ZJU-Leaper\")\n","\n","# JSON file\n","GROUP_JSON_FILE = \"group1.json\"\n","\n","# Labels folder path\n","SOURCE_LABELS_DIR = os.path.join(DATASET_ROOT, \"Label\")\n","\n","# Define images dir\n","SOURCE_IMAGES_DIR = os.path.join(DATASET_ROOT, \"images\")\n","\n","# Define new dataset dir\n","NEW_DATASET_DIR = os.path.join(DATASET_ROOT, \"fabric_dataset\")\n","\n","# Define the train/val dir\n","TRAIN_IMG_DIR = os.path.join(NEW_DATASET_DIR, \"images\", \"train\")\n","TRAIN_LBL_DIR = os.path.join(NEW_DATASET_DIR, \"labels\", \"train\")\n","VAL_IMG_DIR = os.path.join(NEW_DATASET_DIR, \"images\", \"val\")\n","VAL_LBL_DIR = os.path.join(NEW_DATASET_DIR, \"labels\", \"val\")\n","\n","# Define YAML file\n","YAML_FILE_NAME = \"fabric_data.yaml\"\n","YAML_FILE_PATH = os.path.join(NEW_DATASET_DIR, YAML_FILE_NAME)\n","CLASS_NAMES = { 0: \"Defect\" }\n","\n","# Create all the new directories\n","os.makedirs(TRAIN_IMG_DIR, exist_ok=True)\n","os.makedirs(TRAIN_LBL_DIR, exist_ok=True)\n","os.makedirs(VAL_IMG_DIR, exist_ok=True)\n","os.makedirs(VAL_LBL_DIR, exist_ok=True)\n","\n","print(f\"Created directory structure at: {NEW_DATASET_DIR}\")\n","print(\"Ready to proceed.\")"]},{"cell_type":"markdown","source":["# **Create the YAML File**"],"metadata":{"id":"u1H9D7kSEGnp"},"id":"u1H9D7kSEGnp"},{"cell_type":"code","execution_count":null,"id":"901196ca","metadata":{"id":"901196ca"},"outputs":[],"source":["# Create the data dictionary for the YAML file\n","data_yaml = {\n","    'path': os.path.abspath(NEW_DATASET_DIR),\n","    'train': 'images/train',\n","    'val': 'images/val',\n","    'names': CLASS_NAMES\n","}\n","\n","# Write the dictionary\n","try:\n","    with open(YAML_FILE_PATH, 'w') as f:\n","        yaml.dump(data_yaml, f, sort_keys=False)\n","    print(f\"Successfully created {YAML_FILE_NAME} at:\")\n","    print(YAML_FILE_PATH)\n","\n","    print(\"\\n--- YAML File Content ---\")\n","    print(yaml.dump(data_yaml, sort_keys=False))\n","\n","except Exception as e:\n","    print(f\"ERROR: Could not write YAML file. {e}\")"]},{"cell_type":"markdown","source":["# **Data Splitting**"],"metadata":{"id":"VydhWf_MEPn9"},"id":"VydhWf_MEPn9"},{"cell_type":"code","execution_count":null,"id":"cdbb2771","metadata":{"id":"cdbb2771"},"outputs":[],"source":["def copy_files(file_stems, dest_img_dir, dest_lbl_dir):\n","    \"\"\"Helper function to copy images and labels.\"\"\"\n","    copy_count = 0\n","    skip_count = 0\n","    for stem in tqdm(file_stems, desc=f\"Copying to {dest_img_dir}\"):\n","\n","        # Define source paths\n","        img_name = f\"{stem}.jpg\"\n","        lbl_name = f\"{stem}.txt\"\n","\n","        src_img_path = os.path.join(SOURCE_IMAGES_DIR, img_name)\n","        src_lbl_path = os.path.join(SOURCE_LABELS_DIR, lbl_name)\n","\n","        # Define destination paths\n","        dest_img_path = os.path.join(dest_img_dir, img_name)\n","        dest_lbl_path = os.path.join(dest_lbl_dir, lbl_name)\n","\n","        # Check if both files exist before copying\n","        if os.path.exists(src_img_path) and os.path.exists(src_lbl_path):\n","            shutil.copy(src_img_path, dest_img_path)\n","            shutil.copy(src_lbl_path, dest_lbl_path)\n","            copy_count += 1\n","        else:\n","            skip_count += 1\n","\n","    return copy_count, skip_count\n","\n","# Splitting\n","print(\"Starting file splitting...\")\n","json_path = os.path.join(DATASET_ROOT, 'ImageSets', 'Groups', GROUP_JSON_FILE)\n","\n","try:\n","    with open(json_path, 'r') as f:\n","        data = json.load(f)\n","\n","    # Get the 'train' files\n","    train_stems = data['normal']['train'] + data['defect']['train']\n","\n","    # Get the 'test' files\n","    val_stems = data['normal']['test']\n","    if 'test' in data['defect']:\n","        val_stems += data['defect']['test']\n","\n","    print(f\"Loaded {len(train_stems)} files for training.\")\n","    print(f\"Loaded {len(val_stems)} files for validation.\")\n","\n","    # Copy files\n","    train_copied, train_skipped = copy_files(train_stems, TRAIN_IMG_DIR, TRAIN_LBL_DIR)\n","    val_copied, val_skipped = copy_files(val_stems, VAL_IMG_DIR, VAL_LBL_DIR)\n","\n","    print(\"\\n--- Splitting Complete ---\")\n","    print(f\"Training files: {train_copied} copied, {train_skipped} skipped.\")\n","    print(f\"Validation files: {val_copied} copied, {val_skipped} skipped.\")\n","\n","except Exception as e:\n","    print(f\"ERROR: Could not read or process {GROUP_JSON_FILE}. {e}\")"]}],"metadata":{"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}